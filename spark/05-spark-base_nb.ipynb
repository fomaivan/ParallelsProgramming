{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Недостатки MapReduce\n",
    "1) **Минимум 2 итерации записи на HDD** за job'у => не работает в реальном времени. Хотелось бы хранить сплиты в RAM.\n",
    "\n",
    "2) **Парадигма коротко живущих контейнеров.** Вспоминаем лекцию по YARN (приложения на сервис, приложения на задачу...). \n",
    "    * При запуске mapreduce-таски (например, маппера) YARN запустит контейнер\n",
    "    * контейнер умрёт его когда таска завершится. \n",
    "    * При аварии контейнер перезапустится на другой машине. \n",
    "Это удобно но старт-стоп контейнеров даёт Overhead если MapReduce-задач много.\n",
    "\n",
    "3) Нужно писать **очень много кода** (вспоминаем задачу на Join с позапрошлого семинара).\n",
    "\n",
    "4) По сути 1 источник данных - диск (HDFS, локальная ФС клиента... но всё равно диск). Хотелось бы уметь читать / писать в другие источники (базы данных, облачные хранилища).\n",
    "\n",
    "**Итог:** с MapReduce **можно** работать с BigData, но нельзя работать быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **200Х годы**: нам нужна отказоустойчивая система. RAM на серверах мало. Будем сохранять промежуточные данные **на диск**.\n",
    "* **201Х годы**: \n",
    "    - Память становится дешевле и больше. \n",
    "    - Запросы от бизнеса на максимально быструю обработку (real-time).\n",
    " \n",
    "Диск использовать нецелесообразно - возвращаемся к RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Составляющие Spark-экосистемы\n",
    "\n",
    "Spark Написан на Scala, имеет Scala, Java, Python API.\n",
    "\n",
    "1. Spark Core - разбор кода, распределённое выполнение, поддержка отказоустойчивости.\n",
    "2. Аналог \"стандартной библиотеки\":\n",
    "   * Spark SQL - высокоуровненвая обработка с помощью pandas-подобного синтаксиса или SQL.\n",
    "   * Spark Streaming, Spark Structured Streaming - обработка (обновление результатов) данных в real-time\n",
    "   * MLLib - инструментарий для ML. Помимо Spark использует сторонние библиотеки (например Breeze, написанный на Fortran).\n",
    "3. Планировщики:\n",
    "   * Standalone - легковесный Spark на 1 машине. Использует встроенный планировщик\n",
    "   * Может использовать другие планировщики (например YARN, Mesos, Kubernetes).\n",
    "\n",
    "Подробнее **[здесь](https://www.oreilly.com/library/view/learning-spark/9781449359034/ch01.html)**.\n",
    "\n",
    "#### Источники данных\n",
    "![Image](images/datasources.png)\n",
    "\n",
    "В теории можем читать-писать в большое кол-во источников и приёмников данных.\n",
    "\n",
    "На практике:\n",
    "* Есть проблемы при взаимодействии с Hive (подробнее будет на лекции),\n",
    "* И при подключении к Cassandra.\n",
    "* Хорошо взаимодействует с Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура Spark-приложения\n",
    "![Image](images/cluster-overview.png)\n",
    "(https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "\n",
    "1. Driver program - управляющая программа.\n",
    "2. SparkContext - это основной объект, с помощью которого мы взаимодействуем со Spark.\n",
    "3. Cluster manager - планировщик (любой, см. выше).\n",
    "4. Executor - по сути JVM на нодах.\n",
    "\n",
    "В 1-м приближении работает также как и Hadoop. Единственное, контейнеры **долго живущие**. Контейнеры поднимаются 1 раз и умирают когда заканчивается SparkContext. Это позволяет хранить данные **в памяти JVM**. Быстрее RAM только кеши CPU, но это сложно реализуется (ассемблер)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Возможности работы со Spark\n",
    "##### Интерактивный shell\n",
    "\n",
    "1. `spark2-shell` - запускает Scala-оболочку.\n",
    "2. `pyspark2` - python оболочку.\n",
    "\n",
    "В этих оболочках уже имеется готовый SparkContext (переменная `sc`).\n",
    "\n",
    "##### Запуск файла на исполнение\n",
    "`spark2-submit [params] <file>` - можем запускать как jar-файлы, так и коды на Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запуск Spark в Jupyter-ноутбуке:\n",
    "\n",
    "```bash\n",
    "PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_PYTHON=/usr/bin/python3 PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=\"*\" --port=<PORT> --NotebookApp.token=\"<TOKEN>\" --no-browser' pyspark2 --master=yarn --num-executors=<N>\n",
    "```\n",
    " - **PORT** - порт, на котором откроется ноутбук.\n",
    " - **TOKEN** - токен, который нужно будет ввести для входа в Jupyter (любая строка). Не оставляйте токен пустым т.к. в этом случае к вашему ноутбуку смогут подключаться другие пользователи. `--NotebookApp.token=\"<TOKEN>\"` можно не писать, тогда он сгенерится сам, а посмотреть его можно будет с помощью команды `jupyter notebook list`.\n",
    " - **N** - кол-во executors (YARN containers), выделенных на приложение. \n",
    " \n",
    "Подробнее в [Userguide](https://docs.google.com/document/d/1dmb8o3M2ZCsjPq3rJQqd-jNLQhiBXWbWZcTn9aYUAp8/edit).\n",
    " \n",
    "#### Режимы запуска Spark\n",
    "1. **local**. И драйвер, и worker стартуют на 1 машине. Можно указывать число ядер, выделенных на задачу. Например, `local[3]`. Указывать меньше 2 не рекомендуется т.к. всегда запускает 2 процесса: driver, worker.\n",
    "2. **yarn**. Распределённый режим. Здесь можно дополнительно указать `--deploy-mode`. \n",
    "   * `cluster`. Драйвер на мастере либо на ноде. Рекомендуется для прода.\n",
    "   * `client`. Драйвер на клиенте. Проще отлаживаться. Проще работать в интерактивном режиме (сейчас мы работаем в режиме `client`). Но грузит клиент. \n",
    " \n",
    "В аргументах PySpark можно указывать и многе другое, подробнее [здесь](http://spark.apache.org/docs/latest/configuration.html#application-properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mipt-client.atp-fivt.org:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>the pd2023068's spark app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=the pd2023068's spark app>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем изменить конфигурацию SparkContext, правда его придётся перезапустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "conf = sc.getConf().setAppName(\"the {}\\'s spark app\".format(getpass.getuser())).set(\"spark.python.profile\",\"true\")\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\").map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/wiki/en_articles_part\").map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=4>\n",
      "============================================================\n",
      "         45166 function calls (45162 primitive calls) in 0.593 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     8202    0.492    0.000    0.492    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "     4100    0.059    0.000    0.059    0.000 {method 'decode' of 'bytes' objects}\n",
      "     4100    0.015    0.000    0.015    0.000 {method 'strip' of 'str' objects}\n",
      "     4102    0.007    0.000    0.563    0.000 serializers.py:677(loads)\n",
      "     4102    0.006    0.000    0.591    0.000 rdd.py:1055(<genexpr>)\n",
      "     4102    0.004    0.000    0.053    0.000 serializers.py:714(read_int)\n",
      "     4102    0.003    0.000    0.566    0.000 serializers.py:686(load_stream)\n",
      "     4100    0.003    0.000    0.020    0.000 util.py:97(wrapper)\n",
      "     4100    0.002    0.000    0.017    0.000 <ipython-input-6-73a3ecaacb2c>:1(<lambda>)\n",
      "     4102    0.001    0.000    0.001    0.000 {built-in method unpack}\n",
      "        4    0.001    0.000    0.592    0.148 {built-in method sum}\n",
      "        2    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "      6/2    0.000    0.000    0.592    0.296 rdd.py:2498(pipeline_func)\n",
      "        6    0.000    0.000    0.592    0.099 rdd.py:351(func)\n",
      "        2    0.000    0.000    0.593    0.296 worker.py:370(process)\n",
      "        2    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "        2    0.000    0.000    0.592    0.296 rdd.py:1055(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        2    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method dumps}\n",
      "        2    0.000    0.000    0.000    0.000 rdd.py:1046(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        4    0.000    0.000    0.000    0.000 rdd.py:909(func)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method pack}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method iter}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method add}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method len}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Dataset и ленивые вычисления\n",
    "\n",
    "RDD - набор данных, распределённый по партициям (аналог сплитов в Hadoop). Основной примитив работы в Spark. \n",
    "\n",
    "##### Свойства\n",
    "* Неизменяемый. Можем получить либо новый RDD, либо plain object\n",
    "* Итерируемый. Можем делать обход RDD\n",
    "* Восстанавливаемый. Каждая партиция помнит как она была получена (часть графа вычислений) и при утере может быть восстановлена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создать RDD можно:\n",
    "* прочитав данные из источника\n",
    "* получить новый RDD из существующего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\tЯВЛЕНИЕ 1\r\n",
      "Лизанька\tСветает!.. Ах! как скоро ночь минула!\r\n",
      "Лизанька\tВчера просилась спать - отказ,\r\n",
      "Лизанька\t\"Ждем друга\". - Нужен глаз да глаз,\r\n",
      "Лизанька\tНе спи, покудова не скатишься со стула.\r\n",
      "Лизанька\tТеперь вот только что вздремнула,\r\n",
      "Лизанька\tУж день!.. сказать им...\r\n",
      "Лизанька\tГоспода,\r\n",
      "Лизанька\tЭй! Софья Павловна, беда.\r\n",
      "Лизанька\tЗашла беседа ваша за ночь;\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /data/griboedov/gore_ot_uma-1.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/data/griboedov MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/wiki/en_articles_part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идём в [SparkHistory UI](http://localhost:18089/) (для этого нужно пробросить порт 18089). Далее переходим в incompleted applications (приложение не завершилось т.к. SparkContext жив) и видим, что в списке Job пусто.\n",
    "\n",
    "Несмотря на это, сам RDD есть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем кол-во объектов в RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова проверяем UI и... job'а появилась!\n",
    "\n",
    "В Spark'е сть 2 типа операций над RDD:\n",
    "* [трансформации](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations). Преобразуют RDD в новое RDD.\n",
    "* [действия](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions). Преобразуют RDD в обычный объект.\n",
    "\n",
    "Трансформации выполяются **лениво**. При вызове трансформации достраивается граф вычислений и больше ничего не происходит. \n",
    "\n",
    "Реальное выполнение графа происходит при вызове Action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount на Spark\n",
    "\n",
    "Мы уже прочитали данные, теперь попробуем посчитать на них WordCount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# строим граф вычислений\n",
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower()) # приводим к нижнему регистру\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \")) # выделяем слова\n",
    "rdd = rdd.map(lambda x: (x, 1))  # собираем пары (word, 1)\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # суммируем \"1\" с одинаковыми ключами\n",
    "rdd = rdd.sortBy(lambda a: -a[1]) # сортируем по кол-ву встречаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.take(10) # Action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типы трансформаций в Spark\n",
    "\n",
    "![Image](images/stages.png)\n",
    "(https://www.slideshare.net/LisaHua/spark-overview-37479609)\n",
    "\n",
    "* Часть трансформаций (map, flatmap, ...) обрабатывает партиции независимо. Такие трансформации называются *narrow*. \n",
    "* reduce, sortBy аггрегируют данные и используют передачу по сети. Они называются *wide*. \n",
    "   * Wide-трансформации могут менять кол-во партиций.\n",
    "   * По wide-трансформациям происходит деление job'ы на Stages.\n",
    "\n",
    "Stage тоже делится на task'и. 1 task выполняется для одной партиции.\n",
    "\n",
    "**Итак: Task << Stage << Job << Application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Spark есть возможность вывести план job'ы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) /data/wiki/en_articles_part MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      " |  /data/wiki/en_articles_part HadoopRDD[5] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим всего 3 трансформации. Где все остальные?\n",
    "\n",
    "Spark написан на Scala, которая под капотом использует JVM. Чтоб делать вычисления в Python, нужно вытаскивать данные из JVM. А потом возвращаться обратно. Получаем OverHead на сериализацию-десериализацию. Чтоб overhead'ов было меньше, схлопываем узкие трансформации в одну."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Весь пример целиком:** `/home/velkerr/seminars/pd2018/14-15-spark/griboedov.py`\n",
    "\n",
    "Запустим с помощью `spark2-submit griboedov.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1.\n",
    "\n",
    "> При подсчёте отсеять пунктуацию и слова короче 3 символов. \n",
    "При фильтрации можно использовать регулярку: `re.sub(u\"\\\\W+\", \" \", x.strip(), flags=re.U)`.\n",
    "\n",
    "### Задача 2.\n",
    "\n",
    "> Считать только имена собственные. Именами собственными в данном случае будем считать такие слова, у которых 1-я буква заглавная, остальные - прописные.\n",
    "\n",
    "**Решение**: ` /home/velkerr/seminars/pd2018/14-15-spark/griboedov_adv.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аккумуляторы\n",
    "\n",
    "Аналоги счётчиков в Hadoop. \n",
    "* Используется для легковесной аггрегации (без `reduceByKey` и дополнительных shuffle'ов)\n",
    "* Если аккумулятор используется в трансформациях, то нельзя гарантировать консистентность (мы можем с помощью action'a вызвать DAG несколько раз). Можно использовать в `foreach()`.\n",
    "\n",
    "**Объявление:** `cnt = sc.accumulator(start_val)`\n",
    "\n",
    "**Использование:** \n",
    "   * Inline: `foreach(lambda x: cnt.add(x))`\n",
    "   * Или же, с помощью своей функции:\n",
    "    ```python\n",
    "    def count_with_conditions(x):\n",
    "        global cnt\n",
    "        if ...:\n",
    "            cnt += 1\n",
    "\n",
    "    rdd.foreach(lambda x: count_with_conditions(x))\n",
    "    ```\n",
    "\n",
    "**Получение результата:** `cnt.value`\n",
    "\n",
    "Подробнее в [документации](http://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3.\n",
    "\n",
    "> Переделайте задача 2 так, чтоб кол-во имён собственных вычислялось с помощью аккумулятора.\n",
    "\n",
    "**Решение**: ` /home/velkerr/seminars/pd2018/14-15-spark/griboedov_accum.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast-переменные\n",
    "\n",
    "Аналог DistributedCache в Hadoop. Обычно используется когда мы хотим в спарке сделать Map-side join (т.е. имеется 2 датасета: 1 маленький, который и добавляем в broadcast, другой большой)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast = sc.broadcast([\"hadoop\", \"hive\", \"spark\", 'zookeeper', 'kafka']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br_cast.value[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кеширование\n",
    "\n",
    "При перезапуске Action, пересчитывается весь граф вычислений. Это логично т.к. в трансформациях ничего не вычисляется. Полезно это тем, что если за время работы задачи данные обновились (дополнились), нам достаточно просто перевызвать Action.\n",
    "\n",
    "Но если данные не меняются (например, при отладке), такой пересчёт даёт Overhead. Можно **закешировать** часть pipeline. Тогда при след. вызове Action, RDD считается с кеша и пересчёт начнётся с того места, где было кеширование. В History UI все Stage перед этим будут помечены \"Skipped\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/griboedov\")\n",
    "rdd = rdd.map(lambda x: x.strip().lower())\n",
    "rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "rdd = rdd.map(lambda x: (x, 1)).cache() # тут всё хорошо работает, кешируем\n",
    "rdd = rdd.reduceByKey(lambda a, b: a + b) # а тут хотим отладить, поэтому будут перезапуски\n",
    "rdd = rdd.sortBy(lambda a: -a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кешировать можно с помощью двух операций:\n",
    "* `cache()`\n",
    "* `persist(storage_level)`\n",
    "\n",
    "В `persist()` можно указать [StorageLevel](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/storagelevel.html), т.е. на какой носитель кешируем. Можем закешировать в диск, в память, на диск и / или память на несколько нод... или дать возможность Spark'у решить самому (на основе объёма кеша).\n",
    "\n",
    "`cache()` - это простой вариант `persist()`, когда кешируем только в RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практические задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В hdfs в папке `/data/access_logs/big_log` лежит лог в формате\n",
    "\n",
    "* IP-адрес пользователя (`195.206.123.39`),\n",
    "* Далее идут два неиспользуемых в нашем случае поля (`-` и `-`),\n",
    "* Время запроса (`[24/Sep/2015:12:32:53 +0400]`),\n",
    "* Строка запроса (`\"GET /id18222 HTTP/1.1\"`),\n",
    "* HTTP-код ответа (`200`),\n",
    "* Размер ответа (`10703`),\n",
    "* Реферер (источник перехода; `\"http://bing.com/\"`),\n",
    "* Идентификационная строка браузера (User-Agent; `\"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.94 Safari/537.36\"`).\n",
    "\n",
    "Созданы несколько семплов данных разного размера:\n",
    "```\n",
    "3.4 G    10.2 G   /data/access_logs/big_log\n",
    "17.6 M   52.7 M   /data/access_logs/big_log_10000\n",
    "175.4 M  526.2 M  /data/access_logs/big_log_100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример парсинга логов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = \"/data/access_logs/big_log_10000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime as dt\n",
    "\n",
    "log_format = re.compile( \n",
    "    r\"(?P<host>[\\d\\.]+)\\s\" \n",
    "    r\"(?P<identity>\\S*)\\s\" \n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0], \\\n",
    "       request[0], request[1], match.group('status'), match.group('bytes'), \\\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "lines = sc.textFile(DATASET)\n",
    "parsed_logs = lines.map(parseLine).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2й вариант парсинга - с помощью namedtuple\n",
    "\n",
    "```python\n",
    "LogItem = namedtuple(\"LogItem\", \n",
    "                     [\"host\", \"time\", \"method\", \"path\", \"status\", \"length\", \"referer\", \"user_agent\", \"hour\"])\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return LogItem(\"\", \"\", \"\", \"\", \"\", \"\", \"\" ,\"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return LogItem(\n",
    "        host=match.group('host'),\n",
    "        time=match.group('time').split()[0],\n",
    "        method=request[0],\n",
    "        path=request[1],\n",
    "        status=match.group('status'),\n",
    "        length=match.group('bytes'),\n",
    "        referer=match.group('referer'),\n",
    "        user_agent=match.group('user_agent'),\n",
    "        hour=dt.strptime(match.group('time').split()[0],'%d/%b/%Y:%H:%M:%S').hour\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распарсили, получили RDD, закешировали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "log_format = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>\\S*)\\s\"\n",
    "    r\"(?P<user>\\S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")\n",
    "\n",
    "\n",
    "def parseLine(line):\n",
    "    match = log_format.match(line)\n",
    "    if not match:\n",
    "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
    "\n",
    "    request = match.group('request').split()\n",
    "    return (match.group('host'), match.group('time').split()[0],\n",
    "        request[0], request[1], match.group('status'), int(match.group('bytes')),\n",
    "        match.group('referer'), match.group('user_agent'),\n",
    "        dt.strptime(match.group('time').split()[0], '%d/%b/%Y:%H:%M:%S').hour)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark_session = SparkSession.builder.master(\"yarn\").appName(\"501 df\").config(\"spark.ui.port\", \"18089\").getOrCreate()\n",
    "    lines = spark_session.sparkContext.textFile(\"%s\" % sys.argv[1])\n",
    "    parts = lines.map(parseLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "\"Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.read.\n: java.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1107)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:145)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:144)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:144)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:141)\n\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:785)\n\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:655)\n\tat org.apache.spark.sql.SQLContext.read(SQLContext.scala:507)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:97)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:80)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:93)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:121)\n\tat org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:121)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:121)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:120)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:286)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1104)\n\t... 19 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6062820d8c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# edges_df = edges.toDF([\"user\", \"follower\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0medges_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/twitter/twitter_sample_small.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0medges_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"split(data, '\\t') as columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns[0] as user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns[1] as follower\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0medges_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"follower\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataFrameReader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \"\"\"\n\u001b[0;32m--> 791\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrameReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: \"Error while instantiating 'org.apache.spark.sql.internal.SessionStateBuilder':\""
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, lit, when, min, explode\n",
    "\n",
    "def parse_edge(s):\n",
    "    user, follower = s.split(\"\\t\")\n",
    "    return (int(user), int(follower))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n = 400  # number of partitions\n",
    "    \n",
    "    # edges = sc.textFile(\"/data/twitter/twitter_sample_small.txt\").map(parse_edge).cache()\n",
    "    # edges_df = edges.toDF([\"user\", \"follower\"])\n",
    "    \n",
    "    edges_df = spark.read.text(\"/data/twitter/twitter_sample_small.txt\").toDF(\"data\")\n",
    "    edges_df = edges_df.selectExpr(\"split(data, '\\t') as columns\").selectExpr(\"columns[0] as user\", \"columns[1] as follower\")\n",
    "    edges_df = edges_df.select(edges_df[\"user\"].cast(\"int\"), edges_df[\"follower\"].cast(\"int\")).cache()\n",
    "    \n",
    "    x = 12\n",
    "    y = 34\n",
    "    d = 0\n",
    "    distances_df = spark.createDataFrame([(x, x, d)], [\"path\", \"vertex\", \"distance\"])\n",
    "\n",
    "    while True:\n",
    "        print(d)\n",
    "        expanded_df = distances_df.join(edges_df, on=edges_df[\"follower\"]==distances_df[\"vertex\"])\n",
    "        expanded_df = expanded_df.select(\n",
    "            concat(col(\"path\"), lit(\",\"), col(\"user\")).alias(\"path\"), col(\"user\").alias(\"vertex\"),\n",
    "            (col(\"distance\") + 1).alias(\"distance\")\n",
    "        )\n",
    "        distances_df = expanded_df.na.drop().groupBy(\"path\", \"vertex\").agg(min(\"distance\").alias(\"distance\"))\n",
    "\n",
    "        y_count = distances_df.filter(col(\"vertex\") == y).count()\n",
    "        if y_count > 0:\n",
    "            vertices_34 = distances_df.filter(col(\"vertex\") == y).collect()\n",
    "            for vertex in vertices_34:\n",
    "                print(vertex['path'])\n",
    "            break\n",
    "\n",
    "        d += 1\n",
    "        count = distances_df.filter(col(\"distance\") == d ).count()\n",
    "        if count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "12,422,53,52,107,20,23,274,34\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, min, concat\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    edges_df = spark.read.text(\"/data/twitter/twitter_sample_small.txt\").toDF(\"data\")\n",
    "    edges_df = edges_df.selectExpr(\"split(data, '\\t') as columns\").selectExpr(\"columns[0] as user\", \"columns[1] as follower\")\n",
    "    edges_df = edges_df.withColumn(\"user\", col(\"user\").cast(IntegerType())).withColumn(\"follower\", col(\"follower\").cast(IntegerType())).cache()\n",
    "\n",
    "    x = 12\n",
    "    y = 34\n",
    "    d = 0\n",
    "    distances_df = spark.createDataFrame([(x, x, d)], [\"path\", \"vertex\", \"distance\"])\n",
    "\n",
    "    while True:\n",
    "        expanded_df = distances_df.join(edges_df, on=edges_df[\"follower\"] == distances_df[\"vertex\"])\n",
    "        expanded_df = expanded_df.select(\n",
    "            concat(col(\"path\"), lit(\",\"), col(\"user\")).alias(\"path\"), col(\"user\").alias(\"vertex\"),\n",
    "            (col(\"distance\") + 1).alias(\"distance\")\n",
    "        )\n",
    "        distances_df = expanded_df.na.drop().groupBy(\"path\", \"vertex\").agg(min(\"distance\").alias(\"distance\"))\n",
    "\n",
    "        y_count = distances_df.filter(col(\"vertex\") == y).count()\n",
    "        if y_count > 0:\n",
    "            vertices_34 = distances_df.filter(col(\"vertex\") == y).collect()\n",
    "            for vertex in vertices_34:\n",
    "                print(vertex['path'])\n",
    "            break\n",
    "\n",
    "        d += 1\n",
    "        count = distances_df.filter(col(\"distance\") == d).count()\n",
    "        if count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4.\n",
    "> Напишите программу, выводящую на экран TOP5 ip адресов, в которых содержится хотя бы одна цифра 4, с наибольшим количеством посещений.\n",
    "Каждая строка результата должна содержать IP адрес и число посещений, разделенные табуляцией, строки должны быть упорядочены по числу посещений по убыванию, например:\n",
    "```\n",
    "195.206.123.39<TAB>40\n",
    "196.206.123.40<TAB>39\n",
    "191.206.123.41<TAB>38\n",
    "175.206.123.42<TAB>37\n",
    "195.236.123.43<TAB>36\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5.\n",
    ">  Напишите программу, выводящую на экран суммарное распределение количества посетителей по часам (для каждого часа в сутках вывести количество посетителей, пришедших в этот час). Id посетителя = ip + user_agent.\n",
    "Результат должен содержать час в сутках и число посетителей, разделенные табом и упорядоченные по часам. Например:\n",
    "```\n",
    "0<tab>10\n",
    "1<tab>10\n",
    "2<tab>10\n",
    "…..\n",
    "21<tab>30\n",
    "22<tab>20\n",
    "23<tab>10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
